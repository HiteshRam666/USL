# Prac 1: Data Augmentation: What, Why, and How  

## ğŸ“Œ What is Data Augmentation?  
Data Augmentation is a technique used in machine learning and deep learning to artificially increase the size of a dataset by creating modified versions of existing data. This is especially useful in image processing, where transformations such as rotation, flipping, zooming, and brightness adjustments can be applied to generate more training samples.  

## ğŸ¯ Why Use Data Augmentation?  
âœ” **Prevents Overfitting** â€“ Helps the model generalize better by exposing it to more variations of the same data.  
âœ” **Improves Model Performance** â€“ Increases the diversity of training samples, leading to better accuracy.  
âœ” **Compensates for Small Datasets** â€“ Augmentation can help when collecting more real-world data is difficult or expensive.  
âœ” **Enhances Robustness** â€“ The model learns to handle different perspectives, lighting conditions, and distortions.  

## ğŸ“¢ Conclusion  
Data augmentation is a powerful way to improve model accuracy and robustness, particularly in computer vision tasks. By applying transformations such as rotation, zoom, and flipping, models can better generalize to unseen data.  

---

# **Prac 2: Semi-Supervised Learning with Label PropagationğŸ“Œ**  

## **ğŸ›  What is Semi-Supervised Learning?**  
Semi-supervised learning is a machine learning approach that **combines labeled and unlabeled data** to improve model performance. It is useful when:  
- **Labeled data is limited** or expensive to obtain.  
- **Unlabeled data is abundant**, and we want to use it to enhance learning.  
- **The model can make confident predictions** on some unlabeled data, which can then be used for further training.  

In this repository, we use **XGBoost** and **Label Propagation** to iteratively label high-confidence samples from an initially unlabeled dataset.  

---

## **ğŸ” What is Label Propagation?**  
Label Propagation is a **self-training technique** where:  
1. A model is trained on the available **labeled dataset**.  
2. The model predicts labels for **unlabeled samples**.  
3. Only **high-confidence predictions** are added to the labeled dataset.  
4. The process **repeats iteratively** until no more high-confidence samples remain.  

This approach **gradually expands the labeled dataset**, leading to **better model performance** over time.

---

## **ğŸš€ Why Use Semi-Supervised Learning?**  
âœ”ï¸ **Reduces labeling effort** â€“ You donâ€™t need to label all data manually.  
âœ”ï¸ **Utilizes available data** â€“ Makes use of the large pool of unlabeled data.  
âœ”ï¸ **Improves model performance** â€“ The model learns from additional confidently predicted samples.  
âœ”ï¸ **Real-world applications** â€“ Used in NLP, medical imaging, fraud detection, and more.

---

# Prac 3: **Dimensionality Reduction & Feature Selection in Machine Learning**  

## **ğŸ“Œ Introduction**  
Dimensionality reduction is a crucial technique in machine learning used to reduce the number of input variables (features) in a dataset while preserving important information. It helps in improving model performance, reducing computation time, and avoiding overfitting.

In this project, we perform **dimensionality reduction** using **feature selection** and **feature extraction** techniques and train models to evaluate their effectiveness.

---

## **ğŸ§© The Curse of Dimensionality**  
The **Curse of Dimensionality** refers to the challenges that arise when working with high-dimensional data. Some of the major issues include:  

- **Increased computation time:** Training models on high-dimensional data is computationally expensive.  
- **Overfitting:** Models may learn noise instead of patterns due to too many features.  
- **Data sparsity:** In high dimensions, data points become sparse, making distance-based models ineffective.  

### **ğŸ› ï¸ Solution: Dimensionality Reduction**  
To combat the curse of dimensionality, we can apply **dimensionality reduction techniques** like:  

âœ… **Feature Selection:** Selecting the most important features that contribute to the target variable.  
âœ… **Feature Extraction:** Transforming data into a lower-dimensional space while preserving meaningful information.  

---

## **ğŸ”¬ Feature Selection Methods**
Feature selection helps in reducing irrelevant or redundant features while maintaining the predictive power of the dataset.  
This project uses the following techniques:  

1ï¸âƒ£ **Variance Threshold:** Removes features with low variance (almost constant features).  
2ï¸âƒ£ **SelectKBest (ANOVA F-score):** Selects top features based on their statistical importance.  

### **ğŸ“Œ Feature Selection Solutions**
âœ” **Remove redundant features:** Use correlation heatmaps to detect highly correlated features.  
âœ” **Use domain knowledge:** Identify the most relevant features.  
âœ” **Apply automated feature selection methods:** Such as SelectKBest, Recursive Feature Elimination (RFE), and LASSO regression.  

---

## **ğŸ­ Feature Extraction (Dimensionality Reduction Using PCA)**
Feature extraction transforms high-dimensional data into a lower-dimensional space. This project implements:  

ğŸ“Œ **Principal Component Analysis (PCA)**:  
- Reduces dimensionality by projecting data into a new feature space while retaining maximum variance.  
- Helps in visualizing and improving model efficiency.  

---

# Prac 4: Principal Component Analysis (PCA) from Scratch using NumPy  

## ğŸ“Œ What is PCA?  
Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and statistics. It transforms high-dimensional data into a lower-dimensional form while preserving as much variance as possible.  

### ğŸ”¹ Key Concepts:  
- **Dimensionality Reduction**: Helps reduce the number of features while retaining essential patterns.  
- **Feature Extraction**: Converts correlated features into uncorrelated principal components.  
- **Variance Maximization**: The principal components capture the highest variance in the data.  

---

## â“ Why PCA?  
PCA is widely used for:  
âœ” **Reducing Computational Cost**: Lower dimensions mean faster processing.  
âœ” **Handling Multicollinearity**: Removes redundant features.  
âœ” **Visualization**: Helps in plotting high-dimensional data in 2D or 3D.  
âœ” **Noise Reduction**: Filters out irrelevant variations in data.  

---

## ğŸ“Š PCA Calculation Steps  
### Step 1: Standardizing the Data  
Subtract the mean from each feature to center the data around zero.  

### Step 2: Compute the Covariance Matrix  
Calculate the covariance between features to understand their relationships.  

![image](https://github.com/user-attachments/assets/bdaea204-0c17-459a-926b-debd50a24034)

### Step 3: Compute Eigenvalues and Eigenvectors  
Eigenvalues indicate variance captured by each principal component, while eigenvectors define new feature axes.  

![image](https://github.com/user-attachments/assets/c0a3ec2f-ffba-42a7-9720-f7422db52e1c)

### Step 4: Sort Eigenvalues in Descending Order  
Select the top **k** eigenvectors corresponding to the **k** largest eigenvalues.  

### Step 5: Transform Data  
Multiply the original data matrix by the selected eigenvectors to obtain the principal components.  

### ğŸ“Œ **Covariance Matrix, Eigenvalues, and Eigenvectors â€“ Short Explanation**  

#### **1ï¸âƒ£ Covariance Matrix**  
A **covariance matrix** is a square matrix that represents the relationships (covariances) between multiple features in a dataset. It helps measure how two features vary together.  

- If **covariance is positive**, both features increase together.  
- If **covariance is negative**, one increases while the other decreases.  
- If **covariance is zero**, the features are independent.  

#### **2ï¸âƒ£ Eigenvalues and Eigenvectors**  
Eigenvalues and eigenvectors help in transforming data into new principal components.  

âœ… **Eigenvector**: A direction in the feature space along which the data varies the most.  
âœ… **Eigenvalue**: The magnitude (amount of variance) along the corresponding eigenvector.  

---

ğŸš€ **In short**:  
- **Covariance Matrix**: Measures feature relationships.  
- **Eigenvalues**: Show how much variance a principal component captures.  
- **Eigenvectors**: Define the new directions (principal components) in transformed space.  
---

# Prac 5: **ğŸ“Œ K-Means Clustering:**

### **1ï¸âƒ£ What is K-Means?**
K-Means is an **unsupervised machine learning algorithm** used for **clustering data** into **K groups** based on feature similarity. It aims to minimize the distance between data points and their assigned cluster centers (centroids).  

### **2ï¸âƒ£ How K-Means Works? (Workflow)**
The algorithm follows these steps:

1ï¸âƒ£ **Choose the number of clusters (K)**.  
2ï¸âƒ£ **Randomly initialize K centroids** (initial cluster centers).  
3ï¸âƒ£ **Assign each data point to the nearest centroid** (based on Euclidean distance).  
4ï¸âƒ£ **Recompute centroids** by taking the mean of all points in each cluster.  
5ï¸âƒ£ **Repeat steps 3-4 until centroids remain stable** (i.e., no major changes in cluster assignments).  

**ğŸ“Œ Final Output:** Data points are grouped into K clusters.  

---

## **ğŸ“Š Choosing the Right Number of Clusters (K)**
Choosing the optimal number of clusters is crucial in K-Means. Two key methods for this are:  

### **1ï¸âƒ£ Elbow Method**
The **Elbow Method** helps determine the best K by plotting the **Within-Cluster Sum of Squares (WCSS)** for different K values and identifying the "elbow point" where adding more clusters **does not significantly decrease WCSS**.

#### **ğŸ”¹ Within-Cluster Sum of Squares (WCSS)**
WCSS measures how compact the clusters are by calculating the sum of squared distances between each data point and its centroid.  
ğŸ“Œ **Formula:**  

![image](https://github.com/user-attachments/assets/99c377ee-66d8-43b6-abfc-19f7c283b946)


- **Lower WCSS = Better clustering**  

#### **ğŸ“Œ Elbow Method Steps**
1. Compute WCSS for different values of K (e.g., 1 to 10).
2. Plot K vs. WCSS.
3. Find the "elbow point" (where WCSS stops decreasing significantly).
4. Choose K at this elbow.

---

### **2ï¸âƒ£ Silhouette Score**
The **Silhouette Score** measures **how well-separated** the clusters are. It evaluates each point by comparing:
- **Intra-cluster distance (a)** â†’ Distance to points in the same cluster.  
- **Inter-cluster distance (b)** â†’ Distance to the nearest cluster.  

ğŸ“Œ **Formula:**  
![image](https://github.com/user-attachments/assets/40d3ff50-3615-46b8-a934-8ad54b94ccc5)


Silhouette Score helps validate the chosen **K** by checking cluster separation.

---

